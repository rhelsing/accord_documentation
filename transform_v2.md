# Transformations V2

## Introduction

The transformation step transforms data imported into the file pipeline for loading into the Accord IO system or for exporting to a new CSV file. The purpose of this step is to automate the movement of data between otherwise incompatable systems.

Use the transform task in your file pipeline to change values in your data file, based on the rules you establish in the task parameters. Write the rules in JSON format. A simple transformation that changes an abbreviated `employment_type` column to a spelled-out column (e.g., "FT" to "Full Time") might look like this:

**Placeholder** redo this example because it's wrong.
```json
{
  "version": 2,
  "perform": {
    "convert": {
      "employment_type": {
        "FT": "Full Time",
        "PT": "Part Time"
      }
    }
  }
}
```

A more complex transformation may include calculations, field mappings, data queries, and other commands that you can use to build the transformations you need.

## Definitions

The following terms will be used throughout this document:

<dl>
  <dt>Data File</dt>
  <dd>An Excel or csv file containing data. Field names are specified on the first row, with data contained in subsequent rows.</dd>
  <dt>Input</dt>
  <dd>A data file uploaded to Accord IO for acceptance into a File Pipeline</dd>
  <dt>Output</dt>
  <dd>A data file generated by a File Pipeline.</dd>
  <dt>File Pipeline</dt>
  <dd>A pre-defined series of tasks performed on uploaded data files.</dd>
</dl>

## Parameters

The parameters for your transformation are defined in JSON format, with the following top-level objects:

* `{ "save_output": "true" }` Set `save_output` to `true` to save the result of the transformation, or `false` to skip saving the result. This is a useful option if additional tasks are required in your File Pipeline after the Transformation step.
* `{ "version": 2 }` For transformations that follow the specification laid out in this document, make sure `version` is always set to `2`.
* `{ "perform": { ... }}` Specify the transforming actions to take on the Input in the `perform` object. This is the heart of the transformation step. Use the guide below to define the full tranformation.

### Perform Key: `lookup`

Import a set of values from a reference table. The `lookup` key can contain a single JSON object or an array of objects. Each object must contain three keys:

Key Name | Expected Value
-------- | --------------
input_key | The name of the column in the input.
lookup_source | An object with `layout` or `file_drop` as the key name, and the name of the source as the value.
lookup_key | The name of the column in the lookup source.

**Example**
```json
{
  "lookup": {
    "input_key": "employee_id",
    "lookup_source": {
      "layout": "employee"
    },
    "lookup_key": "emp_id"
  }
}
```

**Ryan's Notes**

we should be able to map the values if the headers for the keys being looked up are different from file to lookup file. data_type will likely need more options. Should be able to specify a specific file drop by name or a file pipeline type (and if so, whether to aggregate all of type, or use latest or first, or biggest? or smallest?

### Perform Key: `dedup`

Remove duplicate records from the uploaded data. This key can contain a single object or an array of objects. Each object must contain the following keys:

Key Name | Expected Value
-------- | --------------
column | The name of the input column
option | **Placeholder** (insert possible options here)

**Example**
```json
{
  "dedup": {
    "column": "ssn",
    "option": "accept_first"
  }
}
```

### Perform Keys: `select` and `reject`

Select (query) data from the Input based on a set of criteria, or reject data from the Input based on a set of criteria. These keys can contain a single object or an array of objects. Each additional `select` will ***include*** more records from the Input. Each additional `reject` will ***exclude*** more records from the Input. Each object must contain the following keys:

Key Name | Expected Value
-------- | --------------
column | The name of the column from the Input. Columns generated through a `lookup` or `derive` **placeholder** describe how the alias name will be generated.
eval | Any one of the following criteria names: <ul><li>is</li><li>starts_with</li><li>ends_with</li><li>contains</li><li>is_less_than</li><li>is_greater_than</li><li>matches</li><li>is_type</li><li>is_blank</li><li>is_present</li><li>has_lookup</li><li>lookup</li></ul>
value | The term you are searching for. Note the following restrictions, based on the `eval`:<ul><li>A `matches` eval will expect a Regular Expression **placeholder link** as the `value`. You should use this option only if you know what you're doing.</li><li>A `is_type` eval expects a known data type as the `value`. Possible values include: "Number", "Date", "String", "Name", "SSN", and "Boolean".</li><li>A `is_blank` or `is_present` eval does not require a `value`.</li><li>A `has_lookup` eval can only be used if a `lookup` Key is present. The value should match the value supplied for the `input_key`.</li></ul>

**Example**
```json
{
  "select": [
    {
      "column": "department_name",
      "eval": "is",
      "value": [ "accounting", "finance" ]
    },
    {
      "column": "employee_job_function",
      "eval": "contains",
      "value": "money"
    }
  ],
  "reject": {
    "column": "employee_status",
    "eval": "is_greater_than",
    "value": 25000
  }
}
```

**Ryan's Note on eval name `lookup`:** What does this mean? - I was thinking that this would be a modifier to run any of the above criterion against a field/value on the foriegn lookup row if present. - maybe this could be a column name modifier instead?

**Joe says** I'd like to think more deeply about how to determine column names for columns generated through a lookup or through other means. The column name will be useful as a column name in the resulting output file, and elsewhere in the transformation definition.

**Joe's additional note** I added `contains` as a possible `eval`.

### Perform Key: `convert`

Use a formula to change the data contained in a column. This key can contain a single object or an array of objects. Each object must contain the following keys:

Key Name | Expected Value
-------- | --------------
column | The name of the Input column to convert
formula | A Convert formula object. A list of available conversion functions and their formats is listed, below.

**Conversion Functions**

Function (key) Name | Expected Value Format | Notes
------------------- | --------------------- | -----
concat | [field_name, string, field_name, ...]
add | [field_name, int, field_name, ...]
subtract | [field_name, int, field_name, ...]
multiply | [field_name, int, field_name, ...]
divide | [field_name, int]
exp | [field_name, int]
invert | invert_type or {value_to_flip: flipped_value} | Invert types could be: boolean, yn
self | | Pass the value through unmodified
other | field_name | Pass the value from another field
time-values | \*\*NOW\*\*, \*\*TOMORROW\*\*, {past: time_period}, {future: time_period} | time_period = "2 Months" "1 Year" etc.
custom | ssn, date, name_split

**Example**

```json
{
  "convert": {
    "column": "employment_type",
    "formula": {
      "invert": {
        "FT": "Full Time",
        "PT": "Part Time"
    }
  }
}
```

### Perform Key: `derive`

Create a new column in the Output and populate the column with values based on a formula. This key can contain a single object or an array of objects. Each object must contain the following keys:

Key Name | Expected Value
-------- | --------------
name | The name of the column. This is also known as the header.
formula | A Derive formula object. A list of available derive functions and their formats is listed, below.

**Derive Functions**

Function (key) Name | Expected Value Format | Notes
------------------- | --------------------- | -----
concat | [field_name, string, field_name, ...]
add | [field_name, int, field_name, ...]
subtract | [field_name, int, field_name, ...]
multiply | [field_name, int, field_name, ...]
divide | [field_name, int]
exp | [field_name, int]
invert | invert_type or {value_to_flip: flipped_value} | Invert types could be: boolean, yn
other | field_name | Pass the value from another field
time-values | \*\*NOW\*\*, \*\*TOMORROW\*\*, {past: time_period}, {future: time_period} | time_period = "2 Months" "1 Year" etc.
custom | ssn, date, name_split

**Example**

```json
{
  "derive": {
    "name": "full_name",
    "formula": {
      "concat": [ "first_name", " ", "last_name" ]
    }
  }
}
```

### Perform Key: `reduce`

**Placeholder: clarification needed**

**Joe's comment** Is `reduce` meant to aggregate values across columns, or does `reduce` cause the Output to become like an aggregate report? If it's the latter of the two, then we might also need a `group_by` key. An example Output might look like this for `{ "group_by": "department", "reduce": { "column": "wages", "function": "sum", "ouput_name": "total_wages" } }`:

Department | Total Wages
---------- | -----------
Accounting | $1,319,000
Sales | $8,221,000
etc | ...

Grouping could possibly have multiple levels, e.g., `{ "group_by": [ "department", "job_function" ] }`.

### Perform Key: `map`

Define field mappings between Input and new columns generated through calculations, and Output. **Placeholder** Explain how to name generated columns. Perhaps those names should be user defined?

This key should contain only a single object with the Input/Generated columns as keys and Output columns as values.

**Example**

```json
{
  "map": {
    "ssn": "ssn",
    "name_first": "first_name",
    "name_last": "last_name"
  }
}
```

## Example Transformations

### Loading Employee Data, with a Lookup

This transformation accepts an employee data file as Input and creates a new file with the same data in a different format as Output. This transformation will include union membership status data from a separate File Drop, with the employee ID used as the key that matches the records in each data set. The `employee_ssn` column is also converted to ensure the `ssn` column in the Output is in the proper format for a Social Security number.

```json
{
  "version": 2,
  "save_output": "true",
  "perform": {
    "lookup": {
      "input_key": "employee_id",
      "lookup_source": { "file_drop": "employee_union_status.csv" },
      "lookup_key": "emp_id"
    },
    "convert": {
      "column": "employee_ssn",
      "formula": { "custom": "ssn" }
    },
    "map": {
      "employee_ssn": "ssn",
      "employee_id": "employee_id",
      "first_name": "first_name",
      "middle_name": "middle_name",
      "last_name": "last_name",
      "address_line_1": "address_1",
      "address_line_2": "address_2",
      "city": "city",
      "state": "state",
      "zip_code": "zip",
      "email_address": "email",
      "lookup_employee_union_status_csv_union_status": "union_status"
    }
  }
}
```

### Changing a Yes/No Flag

This transformation accepts a data file that includes a single column that needs to be converted from one standard to another. In this case, Input entries marked with a "Y" are left blank in the Output. Input entries that are blank or marked with an "N" are marked with a "Y" in the Output. All other data is passed through to the Output, unchanged.

```json
{
  "version": 2,
  "save_output": "true",
  "perform": {
    "convert": {
      "column": "waive_flag",
      "formula": { null: "Y", "N": "Y", "Y": null }
    }
  }
}
```

